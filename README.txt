Two types of evaluation:

1) In "prediction" we measure the predictive accuracy of the techniques on a single dataset (for different levels of sparsity -- ie. one dataset with 10% entries removed, one dataset with 20% entries removed etc)

-- since LowRankModels always produces the same imputations for trace and rank models we complete the datasets a single time (we do NOT add any errors -- either via bootstrapping or sampling)

-- since Mice and Amelia always include uncertain estimates, we average the results from 20 imputations each in order to average out the induced errors


2) In "consistency" we measure the spread of accuracy for each of the techniques for random subsets corresponding to different levels of sparsity (ie. 20 random subsets for 10% sparsity, for 20% sparsity etc.)

-- in this case we bootstrap and sample the LowRankModels technique to make it comparable to Amelia and Mice

3) In “all figures and tables”, we have collected all the figures and tables generated by all the analysis in this paper. 

 